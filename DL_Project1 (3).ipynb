{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z-mZnNajlnk"
      },
      "source": [
        "133008100 DL CSCE 636 Project-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHv7PNEbf8y6"
      },
      "source": [
        "Project one. Due: 11:59pm (Central Time) on Wednesday 3/8/2023. (Submit all your code with detailed/clear explanations in a single Jupyter notebook file in Canvas. And email your trained model to csce636projects2023@gmail.com with your name and UIN . 118 points)  \n",
        "\n",
        "This project is on image classification for noisy MNIST dataset. When we add noise to images in the MNIST dataset, the digits in the images become more and more difficult for human to recognize. For example, the images here have increasingly large noise levels. However, interestingly, deep neural networks can still be trained to recognize them relatively well.\n",
        "\n",
        "Your task is to train a good hand-written-digit recognition classifier for the noisy images. Here are the train_images and train_labels. (You can download them, and then use pickle.load(open(path, 'rb')) to open them. They are both tensorflow tensors.)\n",
        "\n",
        "You should submit two things:\n",
        "\n",
        "1) Place all your code in a single Jupyter notebook, and submit it in Canvas. You should accompany your code with clear/detailed explanations, so that we can understand the methods you used.\n",
        "\n",
        "2) Email your trained model to csce636projects2023@gmail.com. (Please make sure to include your name and UIN in the email.) We need to be able to test your model easily in Google CoLab by running a simple line \"test_loss, test_acc = model.evaluate(test_images, test_labels)\" on our holdout test set (test_images, test_labels). (If you need to proprocess the images before running the trained model, please include those preprocess functions in your Jupyter notebook, and explain clearly how to run your code. It needs to be simple to run your codes.)\n",
        "\n",
        "Method of grading: your grade for this project will be equal to:\n",
        "\n",
        "(test_acc + 0.18) x 100\n",
        "\n",
        "For example, if your test accuracy is 0.62, then the grade is 80; and if your test accuracy is 0.9, then the grade is 108. However, if the code in your Jupyter notebook is incomplete, the grade will be 0; and if the explanations in the Jupypter notebook are not clear, then 5 points will be taken away.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSsmlZY-gMUZ"
      },
      "source": [
        "Load Training data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0C7uiy_cb2Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8sd-IfIb34e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "train_data_path=\"/content/636_project1_train_images\"\n",
        "train_labels_path=\"/content/636_project1_train_labels\"\n",
        "\n",
        "train_images=pickle.load(open(train_data_path, 'rb'))\n",
        "train_labels=pickle.load(open(train_labels_path, 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1eGsLF7cTfd",
        "outputId": "b70f9d02-a57f-4392-d598-500c19141d5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEoPjS2XcUM-",
        "outputId": "b006a127-0cb2-4758-9a5f-eac6c581b748"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn1-QLGFg5bH"
      },
      "source": [
        "Reshaping the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFWY7bx7fn33"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urK-jEY8MY-t",
        "outputId": "0a811c6c-6dd0-4bfe-9d52-d3442f8d9c1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([28, 28, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "train_images[10001].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQTjIIB0fn7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a799501-e0a3-43f7-d08a-6020517ad87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/70\n",
            "176/176 [==============================] - 5s 17ms/step - loss: 2.3013 - accuracy: 0.1388 - val_loss: 3.2756 - val_accuracy: 0.1090\n",
            "Epoch 2/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 2.0119 - accuracy: 0.2839 - val_loss: 2.9525 - val_accuracy: 0.1425\n",
            "Epoch 3/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 1.7954 - accuracy: 0.3696 - val_loss: 2.8148 - val_accuracy: 0.1511\n",
            "Epoch 4/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.6085 - accuracy: 0.4451 - val_loss: 1.7019 - val_accuracy: 0.4341\n",
            "Epoch 5/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 1.5181 - accuracy: 0.4739 - val_loss: 1.5008 - val_accuracy: 0.5241\n",
            "Epoch 6/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.4494 - accuracy: 0.5007 - val_loss: 1.4471 - val_accuracy: 0.5321\n",
            "Epoch 7/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.3957 - accuracy: 0.5192 - val_loss: 1.4484 - val_accuracy: 0.5112\n",
            "Epoch 8/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.3532 - accuracy: 0.5354 - val_loss: 1.2850 - val_accuracy: 0.5715\n",
            "Epoch 9/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 1.3224 - accuracy: 0.5473 - val_loss: 1.2731 - val_accuracy: 0.5799\n",
            "Epoch 10/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 1.2922 - accuracy: 0.5542 - val_loss: 1.2180 - val_accuracy: 0.5974\n",
            "Epoch 11/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.2745 - accuracy: 0.5622 - val_loss: 1.1827 - val_accuracy: 0.6058\n",
            "Epoch 12/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.2498 - accuracy: 0.5691 - val_loss: 1.1886 - val_accuracy: 0.5939\n",
            "Epoch 13/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 1.2316 - accuracy: 0.5766 - val_loss: 1.1538 - val_accuracy: 0.6115\n",
            "Epoch 14/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.2204 - accuracy: 0.5803 - val_loss: 1.1268 - val_accuracy: 0.6164\n",
            "Epoch 15/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 1.2033 - accuracy: 0.5872 - val_loss: 1.1257 - val_accuracy: 0.6143\n",
            "Epoch 16/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.1831 - accuracy: 0.5907 - val_loss: 1.1375 - val_accuracy: 0.6129\n",
            "Epoch 17/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.1738 - accuracy: 0.5978 - val_loss: 1.1398 - val_accuracy: 0.6103\n",
            "Epoch 18/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 1.1621 - accuracy: 0.5996 - val_loss: 1.1279 - val_accuracy: 0.6153\n",
            "Epoch 19/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.1454 - accuracy: 0.6060 - val_loss: 1.1125 - val_accuracy: 0.6203\n",
            "Epoch 20/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.1262 - accuracy: 0.6128 - val_loss: 1.1171 - val_accuracy: 0.6206\n",
            "Epoch 21/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.1217 - accuracy: 0.6142 - val_loss: 1.1094 - val_accuracy: 0.6213\n",
            "Epoch 22/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 1.1099 - accuracy: 0.6166 - val_loss: 1.1276 - val_accuracy: 0.6189\n",
            "Epoch 23/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0971 - accuracy: 0.6229 - val_loss: 1.1069 - val_accuracy: 0.6263\n",
            "Epoch 24/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 1.0847 - accuracy: 0.6264 - val_loss: 1.1341 - val_accuracy: 0.6154\n",
            "Epoch 25/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0751 - accuracy: 0.6278 - val_loss: 1.1136 - val_accuracy: 0.6232\n",
            "Epoch 26/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 1.0614 - accuracy: 0.6340 - val_loss: 1.1192 - val_accuracy: 0.6220\n",
            "Epoch 27/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0560 - accuracy: 0.6349 - val_loss: 1.1225 - val_accuracy: 0.6237\n",
            "Epoch 28/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0498 - accuracy: 0.6372 - val_loss: 1.1792 - val_accuracy: 0.6025\n",
            "Epoch 29/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 1.0359 - accuracy: 0.6421 - val_loss: 1.1290 - val_accuracy: 0.6205\n",
            "Epoch 30/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0272 - accuracy: 0.6444 - val_loss: 1.1575 - val_accuracy: 0.6154\n",
            "Epoch 31/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0192 - accuracy: 0.6473 - val_loss: 1.1925 - val_accuracy: 0.6079\n",
            "Epoch 32/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0098 - accuracy: 0.6480 - val_loss: 1.1568 - val_accuracy: 0.6181\n",
            "Epoch 33/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 1.0006 - accuracy: 0.6553 - val_loss: 1.1478 - val_accuracy: 0.6207\n",
            "Epoch 34/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.9884 - accuracy: 0.6575 - val_loss: 1.1689 - val_accuracy: 0.6161\n",
            "Epoch 35/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.9833 - accuracy: 0.6570 - val_loss: 1.2044 - val_accuracy: 0.6135\n",
            "Epoch 36/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9767 - accuracy: 0.6592 - val_loss: 1.1440 - val_accuracy: 0.6237\n",
            "Epoch 37/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 0.9718 - accuracy: 0.6632 - val_loss: 1.1639 - val_accuracy: 0.6186\n",
            "Epoch 38/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9601 - accuracy: 0.6664 - val_loss: 1.1602 - val_accuracy: 0.6177\n",
            "Epoch 39/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9535 - accuracy: 0.6699 - val_loss: 1.2019 - val_accuracy: 0.6102\n",
            "Epoch 40/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9520 - accuracy: 0.6683 - val_loss: 1.2350 - val_accuracy: 0.6038\n",
            "Epoch 41/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.9391 - accuracy: 0.6743 - val_loss: 1.2030 - val_accuracy: 0.6153\n",
            "Epoch 42/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9359 - accuracy: 0.6742 - val_loss: 1.1768 - val_accuracy: 0.6189\n",
            "Epoch 43/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9289 - accuracy: 0.6760 - val_loss: 1.1820 - val_accuracy: 0.6218\n",
            "Epoch 44/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.9228 - accuracy: 0.6800 - val_loss: 1.1791 - val_accuracy: 0.6189\n",
            "Epoch 45/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.9220 - accuracy: 0.6766 - val_loss: 1.2403 - val_accuracy: 0.6122\n",
            "Epoch 46/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.9126 - accuracy: 0.6811 - val_loss: 1.2295 - val_accuracy: 0.6065\n",
            "Epoch 47/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.9077 - accuracy: 0.6834 - val_loss: 1.1861 - val_accuracy: 0.6166\n",
            "Epoch 48/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 0.9057 - accuracy: 0.6822 - val_loss: 1.2447 - val_accuracy: 0.6053\n",
            "Epoch 49/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.8981 - accuracy: 0.6850 - val_loss: 1.2228 - val_accuracy: 0.6113\n",
            "Epoch 50/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8943 - accuracy: 0.6874 - val_loss: 1.2367 - val_accuracy: 0.6121\n",
            "Epoch 51/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8919 - accuracy: 0.6857 - val_loss: 1.2107 - val_accuracy: 0.6181\n",
            "Epoch 52/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8882 - accuracy: 0.6895 - val_loss: 1.2359 - val_accuracy: 0.6143\n",
            "Epoch 53/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8721 - accuracy: 0.6958 - val_loss: 1.2240 - val_accuracy: 0.6160\n",
            "Epoch 54/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8719 - accuracy: 0.6950 - val_loss: 1.2730 - val_accuracy: 0.6040\n",
            "Epoch 55/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8654 - accuracy: 0.6952 - val_loss: 1.2258 - val_accuracy: 0.6131\n",
            "Epoch 56/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.8612 - accuracy: 0.6996 - val_loss: 1.2367 - val_accuracy: 0.6146\n",
            "Epoch 57/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8577 - accuracy: 0.7006 - val_loss: 1.2142 - val_accuracy: 0.6171\n",
            "Epoch 58/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8567 - accuracy: 0.7007 - val_loss: 1.2596 - val_accuracy: 0.6099\n",
            "Epoch 59/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 0.8516 - accuracy: 0.7026 - val_loss: 1.2503 - val_accuracy: 0.6097\n",
            "Epoch 60/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.8493 - accuracy: 0.7019 - val_loss: 1.2303 - val_accuracy: 0.6169\n",
            "Epoch 61/70\n",
            "176/176 [==============================] - 2s 12ms/step - loss: 0.8420 - accuracy: 0.7037 - val_loss: 1.2472 - val_accuracy: 0.6137\n",
            "Epoch 62/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8403 - accuracy: 0.7041 - val_loss: 1.2511 - val_accuracy: 0.6111\n",
            "Epoch 63/70\n",
            "176/176 [==============================] - 2s 13ms/step - loss: 0.8377 - accuracy: 0.7053 - val_loss: 1.2568 - val_accuracy: 0.6087\n",
            "Epoch 64/70\n",
            "176/176 [==============================] - 2s 14ms/step - loss: 0.8288 - accuracy: 0.7089 - val_loss: 1.2670 - val_accuracy: 0.6069\n",
            "Epoch 65/70\n",
            " 53/176 [========>.....................] - ETA: 1s - loss: 0.8083 - accuracy: 0.7161"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-6d5472ca1b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training for fold {fold_no} ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   history = model.fit(train_images[train], \n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \"\"\"\n\u001b[1;32m   1154\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=4, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(train_images, train_labels):\n",
        "  inputs = keras.Input(shape=(28, 28, 1))\n",
        "  x = layers.Conv2D(filters=4, kernel_size=3, activation=\"relu\")(inputs)\n",
        "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Conv2D(filters=8, kernel_size=3, activation=\"relu\")(inputs)\n",
        "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
        "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007),\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"])\n",
        "\n",
        "    # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  history = model.fit(train_images[train],\n",
        "            train_labels[train],\n",
        "            epochs=70,\n",
        "            validation_data=(train_images[test], train_labels[test]),\n",
        "            batch_size=256)\n",
        "\n",
        "  accuracy = history.history[\"accuracy\"]\n",
        "  val_accuracy = history.history[\"val_accuracy\"]\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "  epochs = range(1, len(accuracy) + 1)\n",
        "  plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "  plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "  plt.title(\"Training and validation accuracy\")\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "  plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "  plt.title(\"Training and validation loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnGY861Mfn-4",
        "outputId": "48cfb309-0676-4bef-caaa-60b7adeb1813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_51 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_41 (MaxPoolin  (None, 13, 13, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 13, 13, 32)        0         \n",
            "                                                                 \n",
            " conv2d_52 (Conv2D)          (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_42 (MaxPoolin  (None, 5, 5, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 5, 5, 64)          0         \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1152)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqZ6vs44foCi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "8MlG1lv9koW6",
        "outputId": "60885dc0-9227-4b8a-d666-45af44dec979"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=4, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv2D(filters=8, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007),\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_images,\n",
        "            train_labels,\n",
        "            epochs=100,\n",
        "            validation_split = 0.30,\n",
        "            batch_size=256)\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCwuG0SZ2Grg",
        "outputId": "b91a7677-20ff-4cf6-c9b1-550460e594cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "165/165 [==============================] - 4s 13ms/step - loss: 2.2607 - accuracy: 0.1515 - val_loss: 2.0736 - val_accuracy: 0.2809\n",
            "Epoch 2/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2.0014 - accuracy: 0.2889 - val_loss: 1.9213 - val_accuracy: 0.3353\n",
            "Epoch 3/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.8880 - accuracy: 0.3375 - val_loss: 1.8013 - val_accuracy: 0.3789\n",
            "Epoch 4/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.7723 - accuracy: 0.3817 - val_loss: 1.6677 - val_accuracy: 0.4298\n",
            "Epoch 5/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.6496 - accuracy: 0.4297 - val_loss: 1.6124 - val_accuracy: 0.4362\n",
            "Epoch 6/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.5626 - accuracy: 0.4580 - val_loss: 1.4996 - val_accuracy: 0.4889\n",
            "Epoch 7/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.4944 - accuracy: 0.4870 - val_loss: 1.4143 - val_accuracy: 0.5157\n",
            "Epoch 8/100\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 1.4491 - accuracy: 0.5031 - val_loss: 1.3883 - val_accuracy: 0.5259\n",
            "Epoch 9/100\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 1.4088 - accuracy: 0.5164 - val_loss: 1.3391 - val_accuracy: 0.5482\n",
            "Epoch 10/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.3725 - accuracy: 0.5303 - val_loss: 1.2953 - val_accuracy: 0.5609\n",
            "Epoch 11/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.3351 - accuracy: 0.5444 - val_loss: 1.2700 - val_accuracy: 0.5666\n",
            "Epoch 12/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.3092 - accuracy: 0.5531 - val_loss: 1.2496 - val_accuracy: 0.5800\n",
            "Epoch 13/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.2791 - accuracy: 0.5641 - val_loss: 1.2588 - val_accuracy: 0.5756\n",
            "Epoch 14/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.2607 - accuracy: 0.5687 - val_loss: 1.1941 - val_accuracy: 0.5938\n",
            "Epoch 15/100\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 1.2429 - accuracy: 0.5759 - val_loss: 1.1784 - val_accuracy: 0.6023\n",
            "Epoch 16/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.2227 - accuracy: 0.5844 - val_loss: 1.1596 - val_accuracy: 0.6063\n",
            "Epoch 17/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.2150 - accuracy: 0.5837 - val_loss: 1.1497 - val_accuracy: 0.6121\n",
            "Epoch 18/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.1954 - accuracy: 0.5912 - val_loss: 1.1215 - val_accuracy: 0.6164\n",
            "Epoch 19/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.1793 - accuracy: 0.5972 - val_loss: 1.1125 - val_accuracy: 0.6196\n",
            "Epoch 20/100\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1.1690 - accuracy: 0.6022 - val_loss: 1.1278 - val_accuracy: 0.6161\n",
            "Epoch 21/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.1585 - accuracy: 0.6050 - val_loss: 1.0959 - val_accuracy: 0.6289\n",
            "Epoch 22/100\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 1.1503 - accuracy: 0.6085 - val_loss: 1.0832 - val_accuracy: 0.6311\n",
            "Epoch 23/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.1373 - accuracy: 0.6109 - val_loss: 1.0910 - val_accuracy: 0.6319\n",
            "Epoch 24/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.1311 - accuracy: 0.6145 - val_loss: 1.0727 - val_accuracy: 0.6346\n",
            "Epoch 25/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.1216 - accuracy: 0.6171 - val_loss: 1.0635 - val_accuracy: 0.6367\n",
            "Epoch 26/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.1191 - accuracy: 0.6169 - val_loss: 1.1096 - val_accuracy: 0.6231\n",
            "Epoch 27/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.1159 - accuracy: 0.6199 - val_loss: 1.0512 - val_accuracy: 0.6424\n",
            "Epoch 28/100\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1.1033 - accuracy: 0.6204 - val_loss: 1.0473 - val_accuracy: 0.6429\n",
            "Epoch 29/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0932 - accuracy: 0.6255 - val_loss: 1.0455 - val_accuracy: 0.6430\n",
            "Epoch 30/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0912 - accuracy: 0.6275 - val_loss: 1.0464 - val_accuracy: 0.6412\n",
            "Epoch 31/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0896 - accuracy: 0.6283 - val_loss: 1.0631 - val_accuracy: 0.6393\n",
            "Epoch 32/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0843 - accuracy: 0.6320 - val_loss: 1.0445 - val_accuracy: 0.6472\n",
            "Epoch 33/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0757 - accuracy: 0.6316 - val_loss: 1.0746 - val_accuracy: 0.6352\n",
            "Epoch 34/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0647 - accuracy: 0.6369 - val_loss: 1.0286 - val_accuracy: 0.6534\n",
            "Epoch 35/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0592 - accuracy: 0.6381 - val_loss: 1.0274 - val_accuracy: 0.6511\n",
            "Epoch 36/100\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1.0560 - accuracy: 0.6397 - val_loss: 1.0333 - val_accuracy: 0.6476\n",
            "Epoch 37/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0480 - accuracy: 0.6421 - val_loss: 1.0226 - val_accuracy: 0.6531\n",
            "Epoch 38/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0499 - accuracy: 0.6423 - val_loss: 1.0112 - val_accuracy: 0.6534\n",
            "Epoch 39/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0407 - accuracy: 0.6433 - val_loss: 1.0149 - val_accuracy: 0.6522\n",
            "Epoch 40/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0420 - accuracy: 0.6439 - val_loss: 1.0122 - val_accuracy: 0.6532\n",
            "Epoch 41/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0354 - accuracy: 0.6468 - val_loss: 1.0000 - val_accuracy: 0.6596\n",
            "Epoch 42/100\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1.0285 - accuracy: 0.6498 - val_loss: 1.0153 - val_accuracy: 0.6536\n",
            "Epoch 43/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0281 - accuracy: 0.6481 - val_loss: 1.0150 - val_accuracy: 0.6567\n",
            "Epoch 44/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0287 - accuracy: 0.6486 - val_loss: 0.9976 - val_accuracy: 0.6596\n",
            "Epoch 45/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0231 - accuracy: 0.6499 - val_loss: 0.9924 - val_accuracy: 0.6609\n",
            "Epoch 46/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0162 - accuracy: 0.6526 - val_loss: 0.9987 - val_accuracy: 0.6550\n",
            "Epoch 47/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0137 - accuracy: 0.6510 - val_loss: 1.0090 - val_accuracy: 0.6587\n",
            "Epoch 48/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0090 - accuracy: 0.6557 - val_loss: 1.0377 - val_accuracy: 0.6470\n",
            "Epoch 49/100\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1.0116 - accuracy: 0.6526 - val_loss: 0.9973 - val_accuracy: 0.6582\n",
            "Epoch 50/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0072 - accuracy: 0.6558 - val_loss: 0.9847 - val_accuracy: 0.6608\n",
            "Epoch 51/100\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1.0018 - accuracy: 0.6558 - val_loss: 0.9956 - val_accuracy: 0.6599\n",
            "Epoch 52/100\n",
            "147/165 [=========================>....] - ETA: 0s - loss: 1.0017 - accuracy: 0.6562"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}